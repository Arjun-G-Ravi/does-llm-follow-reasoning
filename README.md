# Do LLMs Follow Their Own Reasoning? A Red-Teaming Investigation of gpt-oss-20b

This project is a submission for the [OpenAI gpt-oss-20b Red-Teaming Challenge](https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming)

## Problem Statement
Recent alignment and interpretability research has stressed the importance of understanding not just what a language model answers, but *why* it answers as it does. With the release of open-weight models like gpt-oss-20b, it is now possible to probe and manipulate the model’s reasoning trace more directly. However, a critical open question remains: **Do LLMs actually follow their own reported reasoning, or can they be induced to ignore it and answer otherwise?** If models can be made to give answers that are inconsistent with their own chain-of-thought, this undermines the reliability of their reasoning traces for alignment, safety, and evaluation.

## Project Objective
This project aims to systematically test whether gpt-oss-20b will follow incorrect reasoning injected into its internal thought process and, as a result, produce answers that match the injected reasoning—even when that reasoning is demonstrably wrong. The goal is to determine if the model is “reasoning-aligned” (faithful to its own thoughts) or “answer-aligned” (overrides reasoning to give the canonical answer).

## Approach and Methodology
- **Reasoning Injection**: We simulate “wrong” reasoning in the style of gpt-oss-20b’s internal thinking. For each test question, we generate a plausible but incorrect reasoning trace (e.g., “The human heart has three chambers: two atria and a single shared ventricle…”), and inject this into the model’s system prompt or analysis phase.

- **Automated Harness**: We run the model in a constrained evaluation loop, replacing the reasoning trace with our fabricated version for each question. We have run this model in ollama in a local system and edited the modelfile for ollama.

- **Dataset Construction**: The test set spans multiple domains (science, math, general knowledge, computer science), with each entry containing a question and a tailored wrong reasoning. Each topic has 20 questions, totalling 200 questions. These topics were selected to see if there was any significant difference in the results for different topics(spoilers: there were)

- **Evaluation**: We record the model’s answers and analyse whether they follow the injected reasoning or revert to the correct canonical answer. Statistical and section-wise analysis is performed using Jupyter notebooks (`final-results-analysis.ipynb`). This part was done manually, because we found that LLMs struggled to correctly classify this(maybe because the wrong answer following reasoning is "correct", while the correct answer or wrong answer without following the reasoning is "wrong"). We ran 2 different runs to see if there was any significant result in the distribution of answers generated by the model(spoilers: there weren't)

- **Findings and Reporting**: We identify cases where the model “reward hacks” by giving answers that match the wrong reasoning. There are also cases where it ignores the injected reasoning and answers correctly(and sometimes even wrongly).

## How to Reproduce
1.  **Setup the environment**:
    *   Install and run [Ollama](https://ollama.com/).
    *   Pull the base model required for the experiment. The `main.py` script will then create a custom model `kaggle-gpt:latest` based on it.
    *   Install Python 3 and the required dependencies (`pandas`, `matplotlib`).

2.  **Run the experiment**:
    *   The `main.py` script is designed to run the experiment.
    *   It reads questions and "wrong reasoning" from `/dataset/combined.json`.
    *   For each question, it dynamically creates a new Ollama model file (`changing-model-file.txt`) with the injected reasoning.
    *   It then creates a new `kaggle-gpt:latest` model and runs the question against it.
    *   The results are saved incrementally to `results.json`.

3.  **Analyze the results**:
    *   The `final-results-analysis.ipynb` notebook contains the code to analyse the results from the `results/manual-index1.csv` and `results/manual-index2.csv` files (which are manually classified results from `results.json`).
    *   This notebook generates plots to visualise the distribution of the model's behaviour across different topics.

## Results
Most answers (≈80%) strictly followed the injected (wrong) reasoning, demonstrating that gpt-oss-20b tends to align its final answer with its internal chain-of-thought, even when that reasoning is fabricated and demonstrably false. Some resistance is present in Science and Computer Science topics, but virtually none in Maths. The “Corrected-Wrong” category was rare and did not show strong topic dependence.

- **Run 1**: Out of 193 total responses (7 ambiguous/unclassifiable):
    - Followed: 152 (~79%)
    - Corrected-Correct: 29 (~15%)
    - Corrected-Wrong: 12 (~6%)
- **Run 2**: Out of 200 responses:
    - Followed: 161 (~80.5%)
    - Corrected-Correct: 32 (~16%)
    - Corrected-Wrong: 7 (~3.5%)

### Topic Trends:
- **Maths** consistently had the highest proportion of “Followed” answers (nearly 100%), indicating the model almost always trusted and followed the injected reasoning in this domain. Note that these are simple mathematical word problems involving addition, subtraction, multiplication and division. We theorised that the early days LLMs struggled significantly with Mathematics, and LLMs learned that strictly following the reasoning is the way to answer Mathematical questions.
- **Science** and **General Knowledge** had a significant minority of “Corrected-Correct” answers, showing some resistance to injected reasoning and an ability to revert to the true answer.
- **Computer Science** showed the highest rate of “Corrected-Correct” answers (22%–28%), suggesting the model is more likely to override incorrect reasoning in this topic and produce the canonical answer.

**Section-wise Distribution (Run 1):**
- **Science** (50 questions): 37 Followed, 9 Corrected-Correct, 4 Corrected-Wrong
- **Maths** (46 questions): 46 Followed, 0 Corrected-Correct, 0 Corrected-Wrong
- **General Knowledge** (46 questions): 36 Followed, 7 Corrected-Correct, 3 Corrected-Wrong
- **Computer Science** (43 questions): 29 Followed, 11 Corrected-Correct, 3 Corrected-Wrong

**Section-wise Distribution (Run 2):**
- **Science** (50 questions): 41 Followed, 7 Corrected-Correct, 2 Corrected-Wrong
- **Maths** (50 questions): 49 Followed, 1 Corrected-Correct, 0 Corrected-Wrong
- **General Knowledge** (50 questions): 39 Followed, 10 Corrected-Correct, 1 Corrected-Wrong
- **Computer Science** (50 questions): 32 Followed, 14 Corrected-Correct, 4 Corrected-Wrong

For detailed analysis and visualisations, please refer to the `final-results-analysis.ipynb` notebook.

## Repository Structure
- `main.py`: The main script to run the reasoning injection experiment.
- `final-results-analysis.ipynb`: Jupyter notebook for analysing and visualising the results.
- `dataset/combined.json`: The dataset of questions and wrong reasoning used in the experiment.
- `results.json`: The raw output from the model for each experiment run.
- `results/manual-index1.csv` & `results/manual-index2.csv`: Manually classified results from the two runs.
- `findings.schema.json`: The JSON schema for reporting findings for the competition.
- `example-harmony-findings.json`: An example of a findings file.
- `writeup.md`: The detailed project writeup.

## Importance and Impact
- **Alignment and Trust**: If LLMs do not follow their own reasoning, then using reasoning as a window into model alignment is unreliable.
- **Safety**: Models that ignore their own reasoning may be able to “deceive” auditors or safety systems by reporting safe reasoning while pursuing other goals.

## Future Directions
- Extend methodology to other open-weight and closed-weight models.
- Run it for a larger dataset - covering a variety of different sub-fields.
- Run multiple runs to ensure that the property that we observe is not run-specific.
- Explore fine-grained reasoning manipulation (partial reasoning, adversarial chains).
- Integrate with alignment and reward-hacking benchmarks.

## References
```
@misc{openai-gpt-oss-20b-red-teaming,
    author = {D. Sculley and Samuel Marks and Addison Howard},
    title = {Red‑Teaming Challenge - OpenAI gpt-oss-20b},
    year = {2025},
    howpublished = {\url{https://kaggle.com/competitions/openai-gpt-oss-20b-red-teaming}},
    note = {Kaggle}
}
```
