{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama model: gpt-oss:20b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# newline after response\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mrepl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mrepl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prompt \u001b[38;5;129;01mor\u001b[39;00m prompt.lower() \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mstream\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     17\u001b[39m p = subprocess.Popen(\n\u001b[32m     18\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m, model, prompt],\n\u001b[32m     19\u001b[39m     stdout=subprocess.PIPE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     bufsize=\u001b[32m1\u001b[39m,\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Ultra‑simple Ollama loop (no argparse, no installs).\n",
    "# Assumes:\n",
    "#   - Ollama is already installed\n",
    "#   - Model (e.g. gpt-oss:20b) already pulled:  ollama pull gpt-oss:20b\n",
    "# Usage:\n",
    "#   python ollama_query.py\n",
    "#   Then type prompts; type exit or quit (or empty line) to stop.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "MODEL = \"gpt-oss:20b\"\n",
    "\n",
    "def stream(prompt: str, model: str = MODEL):\n",
    "    \"\"\"Run `ollama run model prompt` and stream output.\"\"\"\n",
    "    p = subprocess.Popen(\n",
    "        [\"ollama\", \"run\", model, prompt],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "    )\n",
    "    try:\n",
    "        for line in p.stdout:  # type: ignore\n",
    "            print(line, end=\"\")\n",
    "    finally:\n",
    "        p.wait()\n",
    "    if p.returncode != 0:\n",
    "        print(f\"\\n[error] ollama exited with code {p.returncode}\", file=sys.stderr)\n",
    "\n",
    "def repl():\n",
    "    print(f\"Ollama model: {MODEL}\")\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"> \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print()\n",
    "            break\n",
    "        if not prompt or prompt.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        stream(prompt)\n",
    "        print()  # newline after response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    repl()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T16:11:37.968580Z",
     "iopub.status.busy": "2025-08-10T16:11:37.968356Z",
     "iopub.status.idle": "2025-08-10T16:11:37.976010Z",
     "shell.execute_reply": "2025-08-10T16:11:37.975226Z",
     "shell.execute_reply.started": "2025-08-10T16:11:37.968562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hA\u001b[?25l\u001b[?25h cow\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h large\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h hoof\u001b[?25l\u001b[?25hed\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h r\u001b[?25l\u001b[?25humin\u001b[?25l\u001b[?25hant\u001b[?25l\u001b[?25h mamm\u001b[?25l\u001b[?25hal\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h commonly\u001b[?25l\u001b[?25h kept\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h its\u001b[?25l\u001b[?25h mil\u001b[3D\u001b[K\n",
      "milk\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h meat\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h hide\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h C\u001b[?25l\u001b[?25hows\u001b[?25l\u001b[?25h belong\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h species\u001b[?25l\u001b[?25h *\u001b[?25l\u001b[?25hBos\u001b[?25l\u001b[?25h ta\u001b[?25l\u001b[?25hurus\u001b[?25l\u001b[?25h*\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h are\u001b[?25l\u001b[?25h domes\u001b[5D\u001b[K\n",
      "domest\u001b[?25l\u001b[?25hicated\u001b[?25l\u001b[?25h animals\u001b[?25l\u001b[?25h used\u001b[?25l\u001b[?25h worldwide\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h agricultural\u001b[?25l\u001b[?25h purposes\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h They\u001b[?25l\u001b[?25h have\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h \u001b[K\n",
      "distinctive\u001b[?25l\u001b[?25h four\u001b[?25l\u001b[?25h-ch\u001b[?25l\u001b[?25hamber\u001b[?25l\u001b[?25hed\u001b[?25l\u001b[?25h stomach\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h allows\u001b[?25l\u001b[?25h them\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h digest\u001b[?25l\u001b[?25h tough\u001b[?25l\u001b[?25h plant\u001b[?25l\u001b[?25h m\u001b[1D\u001b[K\n",
      "material\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h they\u001b[?25l\u001b[?25h are\u001b[?25l\u001b[?25h known\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h their\u001b[?25l\u001b[?25h doc\u001b[?25l\u001b[?25hile\u001b[?25l\u001b[?25h nature\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h social\u001b[?25l\u001b[?25h behavior\u001b[?25l\u001b[?25h wi\u001b[2D\u001b[K\n",
      "within\u001b[?25l\u001b[?25h her\u001b[?25l\u001b[?25hds\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
      "\n",
      "\u001b[?25l\u001b[?25h"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "os.system(\"ollama serve &\")\n",
    "!ollama run 'gpt-oss:20b' /what is a cow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def query_ollama_streaming(prompt, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"Stream output from Ollama in real-time\"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['ollama', 'run', model, prompt],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        bufsize=1,  # Line buffered\n",
    "        universal_newlines=True\n",
    "    )\n",
    "    \n",
    "    # Stream stdout in real-time\n",
    "    for line in process.stdout:\n",
    "        print(line, end='', flush=True)  # Print without adding extra newline\n",
    "    \n",
    "    # Wait for process to complete and get return code\n",
    "    process.wait()\n",
    "    \n",
    "    # Handle any errors\n",
    "    if process.returncode != 0:\n",
    "        stderr_output = process.stderr.read()\n",
    "        print(f\"\\nError: {stderr_output}\", file=sys.stderr)\n",
    "    \n",
    "    return process.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mquery_ollama_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain machine learning in simple terms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mquery_ollama_streaming\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m      3\u001b[39m process = subprocess.Popen(\n\u001b[32m      4\u001b[39m     [\u001b[33m'\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m'\u001b[39m, model, prompt],\n\u001b[32m      5\u001b[39m     stdout=subprocess.PIPE,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     universal_newlines=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Stream stdout in real-time\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Print without adding extra newline\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Wait for process to complete and get return code\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "query_ollama_streaming(\"Explain machine learning in simple terms\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13289915,
     "sourceId": 106949,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
